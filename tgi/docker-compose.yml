version: "3.8"
services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    runtime: nvidia
    ports:
      - "8080:80"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - MODEL_ID=meta-llama/Llama-3.2-1B
      - MAX_NEW_TOKENS=256
      - DEVICE=gpu
      - QUANTIZE=bitsandbytes-nf4
      - CUDA_GRAPHS=0
      - DISABLE_CUSTOM_KERNELS=true
      - SHARDED=false
      - NUM_SHARD=1
